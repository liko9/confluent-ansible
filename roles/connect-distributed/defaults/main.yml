kafka:
  connect:
    distributed:
      config_file: /etc/kafka/connect-distributed.properties
      logging_config_file: /etc/kafka/connect-distributed.logging.properties
      #      systemd_file: /usr/lib/systemd/system/confluent-connect-distributed.service
      service_name: confluent-kafka-connect
      plugin_path:
        - /usr/share/java
      config:
        consumer.interceptor.classes: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
        producer.interceptor.classes: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
        config.storage.replication.factor: 3
        config.storage.topic: connect-configs
        group.id: connect-cluster
        internal.key.converter: org.apache.kafka.connect.json.JsonConverter
        internal.key.converter.schemas.enable: false
        internal.value.converter: org.apache.kafka.connect.json.JsonConverter
        internal.value.converter.schemas.enable: false
        offset.flush.interval.ms: 10000
        offset.storage.replication.factor: 3
        offset.storage.topic: connect-offsets
        status.storage.replication.factor: 3
        status.storage.topic: connect-status
        key.converter: io.confluent.connect.avro.AvroConverter
        value.converter: io.confluent.connect.avro.AvroConverter
      environment:
        KAFKA_HEAP_OPTS: "-Xmx1000M"
        LOG_DIR: /var/log/connect-distributed
      systemd:
        enabled: yes
        state: started
        LimitNOFILE: 128000
        TimeoutStopSec: 300
        RestartSec: 5
